{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "#from tinydb import TinyDB\n",
    "#import requests\n",
    "#from bs4 import BeautifulSoup\n",
    "#from urllib.request import urlopen\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "workDir = '/home/lefko/git/ut-health-project'\n",
    "dbfile = workDir + '/db/tweets.csv'"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the JSON file can be accessed\n",
    "try:\n",
    "    f = open(dbfile)\n",
    "except IOError:\n",
    "    print(\"File not accessible\")\n",
    "finally:\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCitiesPerState(state):\n",
    "    '''\n",
    "        @param state must begin with an uppercase letter\n",
    "        scrape the list of cities and towns in the respective state\n",
    "    '''\n",
    "    raise DeprecationWarning('Not working, sadly.')\n",
    "    cities = []\n",
    "    url = 'https://en.wikipedia.org/wiki/List_of_cities_and_towns_in_' + state\n",
    "    print('url to scrape:', url)\n",
    "    website = requests.get(url)\n",
    "    # load the retrieved site with BS\n",
    "    soup = BeautifulSoup(website.text,'html.parser')\n",
    "    \n",
    "    # cities/towns are stored in an element with class 'wikitable sortable'\n",
    "    cities_table = soup.find_all(\"table\", class_ = \"wikitable sortable\")\n",
    "    print(len(cities_table))\n",
    "    '''\n",
    "    cities_links = cities_table.findAll('a') # city names are links in the table\n",
    "    for link in cities_links:\n",
    "        cities.append(link.get('title'))\n",
    "    '''\n",
    "    return cities\n",
    "\n",
    "def readCitiesFromFile(state):\n",
    "    cities = []\n",
    "    with open(workDir + '/data/cities_' + state + '.txt') as txt:\n",
    "        for line in txt:\n",
    "            cities.append(line.strip().lower())\n",
    "\n",
    "    return pd.DataFrame(cities, columns = ['city'])\n",
    "\n",
    "def readKeywords():\n",
    "    keywords = []\n",
    "\n",
    "    with open(workDir + '/data/keywords.txt') as txt:\n",
    "        for line in txt:\n",
    "            keywords.append(line.strip().lower())\n",
    "\n",
    "    print(keywords)\n",
    "    return keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_colorado = readCitiesFromFile('co')\n",
    "cities_california = readCitiesFromFile('ca')\n",
    "cities_massachussetts = readCitiesFromFile('ma')\n",
    "cities_all = cities_california + cities_colorado + cities_massachussetts\n",
    "print(cities_colorado[0:10])\n",
    "print(cities_california[0:10])\n",
    "print(cities_massachussetts[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(dbfile, names=['tweet_id', 'created', 'loc', 'text'], header=None)\n",
    "tweets = tweets[tweets['loc'].isnull() == False] # sort out NaN places\n",
    "tweets = tweets[tweets['loc'].str.contains('\\d') == False]\n",
    "tweets = tweets.applymap(lambda s:s.lower() if type(s) == str else s)\n",
    "print(len(tweets), 'tweets loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Tweets are saved in a JSON. Read this file and convert it's contents to a pandas dataframe.\n",
    "\n",
    "def accessDB():\n",
    "    db = TinyDB(dbfile)\n",
    "    return db.all()\n",
    "\n",
    "\n",
    "tweets = accessDB()\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for i, tweet in enumerate(tweets):\n",
    "    df = df.append(pd.DataFrame(tweet, index = [i]))\n",
    "\n",
    "print(len(df), 'tweets loaded!')\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweets.head(15))\n",
    "print(tweets.tail(15))"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How unique are the users locations?\n",
    "print(tweets['loc'].unique())\n",
    "#print(tweets['loc'].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for some real location names\n",
    "tweets[tweets['loc'].isin(cities_all) == True]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These tweets contain locations, either real or fake\n",
    "tweets[tweets['loc'].str.contains('|'.join([' ma', ' co', ' ca']))]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average tweet length\n",
    "np.mean(tweets['text'].str.len())"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet tokenization and stopword removal\n",
    "_stopwords = ['https', 'co', 'ct'] + stopwords.words('english')\n",
    "_keywords = readKeywords()\n",
    "def processTweet(tweet):\n",
    "    lem = WordNetLemmatizer()\n",
    "\n",
    "    tweet = [word for word in word_tokenize(tweet) if word.isalpha() == True]\n",
    "    tweet = [lem.lemmatize(word,'v') for word in tweet]\n",
    "    return [word for word in tweet if word not in _stopwords]\n",
    "\n",
    "'''\n",
    "A vocabulary in Natural Language Processing is a list of all speech segments available for the model. In our case, this includes all the words resident in the Training set we have, as the model can make use of all of them relatively equally â€” at this point, to say the least\n",
    "'''\n",
    "def buildVocab(tokenizedData):\n",
    "    wordlist = nltk.FreqDist(tokenizedData)\n",
    "    #wordlist.plot(50, cumulative = True)\n",
    "    word_features = wordlist.keys()\n",
    "    \n",
    "    return word_features\n",
    "\n",
    "def filterKeywords(tweet):\n",
    "    if any(x in processTweet(tweet) for x in _keywords):\n",
    "        print(tweet)\n",
    "    #if(tweet in _keywords):\n",
    "    #    print(tweet)\n",
    "\n",
    "processed_tweets = []\n",
    "for tweet in tweets['text']:\n",
    "    filterKeywords(tweet)\n",
    "    #tweet_tokens = processTweet(tweet)\n",
    "    #for token in tweet_tokens:\n",
    "    #    processed_tweets.append(token)\n",
    "\n",
    "#print(processed_tweets[0:10])\n",
    "#print(buildVocab(processed_tweets))\n",
    "#buildVocab(processed_tweets[:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}