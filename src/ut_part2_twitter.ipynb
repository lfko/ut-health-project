{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464bitlficonda5c33be88125242aca6d772c29aa3d2b6",
   "display_name": "Python 3.7.4 64-bit ('lfi': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "[nltk_data] Downloading package punkt to /home/lefko/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /home/lefko/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /home/lefko/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Folium\n",
    "import folium \n",
    "\n",
    "# geotext\n",
    "from geotext import GeoText\n",
    "import geocoder\n",
    "\n",
    "# SciPy\n",
    "#from scipy import stats\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# fuzzywuzzy\n",
    "from fuzzywuzzy import fuzz # install python-Levenshtein afterwards for speeding up things\n",
    "\n",
    "# gensim\n",
    "from gensim.models import TfidfModel\n",
    "from gensim import corpora, models\n",
    "from gensim.similarities import Similarity\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "\n",
    "# sklearn\n",
    "#from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global properties\n",
    "workDir = '/home/lefko/git/ut-health-project'\n",
    "dbfile = workDir + '/db/tweets.csv'\n",
    "states = workDir + '/data/us-states-abbr.csv'\n",
    "\n",
    "_stopwords = ['AT_USER', 'URL'] + stopwords.words('english')\n",
    "_lem = WordNetLemmatizer()\n",
    "sample = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the tweets file can be accessed\n",
    "try:\n",
    "    f = open(dbfile)\n",
    "except IOError:\n",
    "    print(\"File not accessible\")\n",
    "finally:\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    some utility function\n",
    "'''\n",
    "def readCitiesFromFile(state):\n",
    "    cities = []\n",
    "    with open(workDir + '/data/cities_' + state + '.txt') as txt:\n",
    "        for line in txt:\n",
    "            cities.append(line.strip().lower())\n",
    "\n",
    "    return pd.DataFrame(cities, columns = ['city'])\n",
    "\n",
    "def readKeywords():\n",
    "    keywords = []\n",
    "\n",
    "    with open(workDir + '/data/keywords.txt') as txt:\n",
    "        for line in txt:\n",
    "            keywords.append(line.strip().lower())\n",
    "\n",
    "    print(keywords)\n",
    "    return keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_colorado = readCitiesFromFile('co') # Colorado cities\n",
    "cities_california = readCitiesFromFile('ca') # California cities\n",
    "cities_massachussetts = readCitiesFromFile('ma') # Massachussetts cities\n",
    "cities_all = cities_california + cities_colorado + cities_massachussetts # all 3 of them combined\n",
    "\n",
    "states_abbr = pd.read_csv(states, sep=',') # official abbreviations for the states\n",
    "states_abbr = states_abbr.applymap(lambda s:s.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['AT_USER', 'URL', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'al', 'ak', 'az', 'ar', 'ca', 'co', 'ct', 'de', 'fl', 'ga', 'hi', 'id', 'il', 'in', 'ia', 'ks', 'ky', 'la', 'me', 'md', 'ma', 'mi', 'mn', 'ms', 'mo', 'mt', 'ne', 'nv', 'nh', 'nj', 'nm', 'ny', 'nc', 'nd', 'oh', 'ok', 'or', 'pa', 'ri', 'sc', 'sd', 'tn', 'tx', 'usa', 'ut', 'vt', 'va', 'wa', 'wv', 'wi', 'wy']\n"
    }
   ],
   "source": [
    "abbrs = []\n",
    "for i in range(len(states_abbr)):\n",
    "    abbrs.append(states_abbr.iloc[:, 1][i].strip())\n",
    "    #abbrs.append(states_abbr.iloc[:, 1][i])\n",
    "\n",
    "_stopwords += abbrs # add the abbreviations to the list of stopwords aswell\n",
    "print(_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "CPU times: user 972 ms, sys: 76.1 ms, total: 1.05 s\nWall time: 919 ms\ntweets dataframe created!\nCPU times: user 632 ms, sys: 20 ms, total: 652 ms\nWall time: 652 ms\nall strings are lowered\n"
    }
   ],
   "source": [
    "%time tweets = pd.read_csv(dbfile, names=['tweet_id', 'created', 'loc', 'text'], header=None) # create a new DataFrame, which holds the tweets\n",
    "\n",
    "if(sample > 0):\n",
    "    tweets = tweets[0:sample]\n",
    "\n",
    "print('tweets dataframe created!')\n",
    "tweets = tweets[tweets['loc'].isnull() == False] # sort out NaN places\n",
    "tweets = tweets[tweets['loc'].str.contains('\\d') == False] # sort out tweets where the location contains numbers\n",
    "tweets = tweets.drop(['tweet_id', 'created'], axis=1) # they are not needed, actually\n",
    "%time tweets = tweets.applymap(lambda s:s.lower() if type(s) == str else str(s)) # lower all strings\n",
    "print('all strings are lowered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "421829\n"
    }
   ],
   "source": [
    "# How many tweets are there in total now?\n",
    "print(len(tweets))\n",
    "sample = len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "40217 unique locations found\n76.60094493266229 average length of a tweet\n"
    }
   ],
   "source": [
    "# How many unique locations?\n",
    "print(len(tweets['loc'].unique()), 'unique locations found')\n",
    "# Average tweet length?\n",
    "print(np.mean(tweets['text'].str.len()), 'average length of a tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>loc</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>511831</th>\n      <td>columbia sc</td>\n      <td>i wish the whole twitter followed me.  i got l...</td>\n    </tr>\n    <tr>\n      <th>511832</th>\n      <td>los angeles ca</td>\n      <td>cali bash after party (saturday) 21+ inside tr...</td>\n    </tr>\n    <tr>\n      <th>511833</th>\n      <td>seattle wa</td>\n      <td>@thatsbishen @kobealfaro hahahahahaha she be h...</td>\n    </tr>\n    <tr>\n      <th>511834</th>\n      <td>between conspire inspire</td>\n      <td>@mac_marceau @archeedebunker @takethatgods @di...</td>\n    </tr>\n    <tr>\n      <th>511835</th>\n      <td>https twitter com marvin hil</td>\n      <td>@birdseye1 @outlawedmind @dannaja64danny @jaac...</td>\n    </tr>\n    <tr>\n      <th>511836</th>\n      <td>toronto ontario</td>\n      <td>@chiefpeggtfs @raptors @superfan_nav @toronto_...</td>\n    </tr>\n    <tr>\n      <th>511837</th>\n      <td>farmington mi</td>\n      <td>happiest birthday to the lovely @robinpoldark ...</td>\n    </tr>\n    <tr>\n      <th>511838</th>\n      <td>philadelphia pa</td>\n      <td>@georgemonbiot oh can that mockumentary of you...</td>\n    </tr>\n    <tr>\n      <th>511839</th>\n      <td>california usa</td>\n      <td>@anchor making a trailer for your podcast is e...</td>\n    </tr>\n    <tr>\n      <th>511840</th>\n      <td>atlanta ga</td>\n      <td>whatâ€™s a bra?</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                                 loc  \\\n511831                   columbia sc   \n511832                los angeles ca   \n511833                    seattle wa   \n511834      between conspire inspire   \n511835  https twitter com marvin hil   \n511836               toronto ontario   \n511837                 farmington mi   \n511838               philadelphia pa   \n511839                california usa   \n511840                    atlanta ga   \n\n                                                     text  \n511831  i wish the whole twitter followed me.  i got l...  \n511832  cali bash after party (saturday) 21+ inside tr...  \n511833  @thatsbishen @kobealfaro hahahahahaha she be h...  \n511834  @mac_marceau @archeedebunker @takethatgods @di...  \n511835  @birdseye1 @outlawedmind @dannaja64danny @jaac...  \n511836  @chiefpeggtfs @raptors @superfan_nav @toronto_...  \n511837  happiest birthday to the lovely @robinpoldark ...  \n511838  @georgemonbiot oh can that mockumentary of you...  \n511839  @anchor making a trailer for your podcast is e...  \n511840                                      whatâ€™s a bra?  "
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the 10 latest tweets\n",
    "tweets.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We already see a problem here: locations are not always real or useful at all. We'll have to deal with this accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NLP Tweet processing\n",
    "### Tokenization, Stopwords and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "CPU times: user 2min 30s, sys: 220 ms, total: 2min 31s\nWall time: 2min 31s\n"
    }
   ],
   "source": [
    "%%time\n",
    "# tweet tokenization and stopword removal\n",
    "def processTweet(tweet_text):\n",
    "    tweet_text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', 'URL', tweet_text) # remove URLs\n",
    "    tweet_text = re.sub('@[^\\s]+', 'AT_USER', tweet_text) # remove usernames\n",
    "    tweet_text = re.sub(r'#([^\\s]+)', r'\\1', tweet_text) # remove the # in #hashtag\n",
    "    tweet_text = [word for word in word_tokenize(tweet_text) if word.isalpha() == True and word not in _stopwords] # tokenize the text\n",
    "    tweet_text = [_lem.lemmatize(word,'v') for word in tweet_text] # lemmatize\n",
    "    return tweet_text\n",
    "\n",
    "tweets['processed_text']=''\n",
    "processed_tweets = []\n",
    "for i in range(sample):\n",
    "    tweet = tweets.iloc[i]\n",
    "    processed_tweets.append(processTweet(tweet['text']))\n",
    "    #tweet.iloc[i]['processed_text'] = processTweet(tweet['text'])\n",
    "\n",
    "tweets['processed_text'][0:sample] = processed_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "CPU times: user 353 Âµs, sys: 1e+03 ns, total: 354 Âµs\nWall time: 302 Âµs\n"
    },
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>loc</th>\n      <th>text</th>\n      <th>processed_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>all over the place</td>\n      <td>@jordanmastagni3 \\r\\n\\r\\nthank you for the fol...</td>\n      <td>[thank, follow, jordan]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>los angeles ca</td>\n      <td>can you recommend anyone for this #skilledtrad...</td>\n      <td>[recommend, anyone, skilledtrade, job, palmdal...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>redding ca</td>\n      <td>rehearsals have been going well. weâ€™re set to ...</td>\n      <td>[rehearsals, go, well, set, film, rest, new, m...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>pine and buckeye az</td>\n      <td>2013 please read it. let's media spread their ...</td>\n      <td>[please, read, let, media, spread, lie, well, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>montclair ca</td>\n      <td>@gennefer @davidgrosstv very cool!!!!</td>\n      <td>[cool]</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>hanford ca</td>\n      <td>@mholder95 i thought it was hilarious. if you ...</td>\n      <td>[think, hilarious, laugh, lose, mind, humor, s...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>hollister ca</td>\n      <td>these days i balance all the hate out with the...</td>\n      <td>[days, balance, hate, love]</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>marina del rey ca</td>\n      <td>works out good for the canes since we just got...</td>\n      <td>[work, good, can, since, get, beat, bobby, bou...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>palm springs calif</td>\n      <td>impeach nancy?</td>\n      <td>[impeach, nancy]</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>laguna beach ca</td>\n      <td>getting a little love jeanninesrestaurants whi...</td>\n      <td>[get, little, love, jeanninesrestaurants, spur...</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                   loc                                               text  \\\n0   all over the place  @jordanmastagni3 \\r\\n\\r\\nthank you for the fol...   \n1       los angeles ca  can you recommend anyone for this #skilledtrad...   \n2           redding ca  rehearsals have been going well. weâ€™re set to ...   \n3  pine and buckeye az  2013 please read it. let's media spread their ...   \n4         montclair ca              @gennefer @davidgrosstv very cool!!!!   \n5           hanford ca  @mholder95 i thought it was hilarious. if you ...   \n6         hollister ca  these days i balance all the hate out with the...   \n7    marina del rey ca  works out good for the canes since we just got...   \n8   palm springs calif                                     impeach nancy?   \n9      laguna beach ca  getting a little love jeanninesrestaurants whi...   \n\n                                      processed_text  \n0                            [thank, follow, jordan]  \n1  [recommend, anyone, skilledtrade, job, palmdal...  \n2  [rehearsals, go, well, set, film, rest, new, m...  \n3  [please, read, let, media, spread, lie, well, ...  \n4                                             [cool]  \n5  [think, hilarious, laugh, lose, mind, humor, s...  \n6                        [days, balance, hate, love]  \n7  [work, good, can, since, get, beat, bobby, bou...  \n8                                   [impeach, nancy]  \n9  [get, little, love, jeanninesrestaurants, spur...  "
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time tweets.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Vectorization\n",
    "Let's represent our tweets as numerical values, i.e. vectors in a vector space\n",
    "\n",
    "Note that, a â€˜tokenâ€™ typically means a â€˜wordâ€™. A â€˜documentâ€™ can typically refer to a â€˜sentenceâ€™ or â€˜paragraphâ€™ and a â€˜corpusâ€™ is typically a â€˜collection of documents as a bag of wordsâ€™.\n",
    "\n",
    "https://dev.to/coderasha/compare-documents-similarity-using-python-nlp-4odp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01/08 not needed\n",
    "#%%time\n",
    "\n",
    "# https://stackoverflow.com/questions/35867484/pass-tokens-to-countvectorizer\n",
    "#def usePreprocessed(doc):\n",
    "#    return doc\n",
    "\n",
    "#vectorizer = CountVectorizer(tokenizer=usePreprocessed,preprocessor=None, lowercase=False) # CV does all the tokenization and stopword removal all by itself already\n",
    "#tfidf = TfidfVectorizer()\n",
    "#tweet_vecs = vectorizer.fit_transform(tweets['processed_text']) # sparse matrix\n",
    "#print(vectorizer.get_feature_names())\n",
    "#print(tweet_vecs.toarray())\n",
    "#print(tweet_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "CPU times: user 8.55 s, sys: 144 ms, total: 8.7 s\nWall time: 8.83 s\n"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "'''\n",
    "In order to work on text documents, Gensim requires the words (aka tokens) be converted to unique ids. So, Gensim lets you create a Dictionary object that maps each word to a unique id. Let's convert our sentences to a [list of words] and pass it to the corpora.Dictionary() object.\n",
    "'''\n",
    "corpDict = corpora.Dictionary(tweets['processed_text'])\n",
    "corpus = [corpDict.doc2bow(text) for text in tweets['processed_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Corpus is a Bag of Words. It is a basically object that contains the word id and its frequency in each document (just lists the number of times each word occurs in the sentence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Term Frequency â€“ Inverse Document Frequency(TF-IDF) is also a bag-of-words model but unlike the regular corpus, TFIDF down weights tokens (words) that appears frequently across documents.\n",
    "\n",
    "Tf-Idf is calculated by multiplying a local component (TF) with a global component (IDF) and optionally normalizing the result to unit length. Term frequency is how often the word shows up in the document and inverse document frequency scales the value by how rare the word is in the corpus. In simple terms, words that occur more frequently across the documents get smaller weights.\n",
    "'''\n",
    "tf_idf = TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup Similarity measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['ill', 'sick', 'cold', 'flu', 'influenza', 'disease', 'weak', 'cough', 'headache', 'stomach']\n10 keywords loaded!\n"
    }
   ],
   "source": [
    "# load some illness-related keywords\n",
    "kws = readKeywords()\n",
    "print(len(kws), 'keywords loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "CPU times: user 1min 41s, sys: 687 ms, total: 1min 41s\nWall time: 1min 43s\n"
    }
   ],
   "source": [
    "%%time\n",
    "# We are storing index matrix in 'data' directory\n",
    "index_temp = get_tmpfile('index')\n",
    "sims = Similarity(index_temp,tf_idf[corpus], num_features=len(corpDict))\n",
    "\n",
    "# update the already existing dictionary with the keywords\n",
    "kws_bow = corpDict.doc2bow(kws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "CPU times: user 547 ms, sys: 4 ms, total: 551 ms\nWall time: 557 ms\n"
    }
   ],
   "source": [
    "%%time\n",
    "# perform a similarity query against the corpus\n",
    "kws_bow_tf_idf = tf_idf[kws_bow]\n",
    "# print(document_number, document_similarity)\n",
    "#print('Comparing Result:', sims[kws_bow_tf_idf]) \n",
    "similarities = sims[kws_bow_tf_idf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "processed_text  \\\n50                                       [sick, everyone]   \n269                                            [headache]   \n1034    [vet, rescue, cough, think, kennel, cough, go,...   \n1384                                   [die, stomach, do]   \n1406                    [headache, morning, whole, level]   \n...                                                   ...   \n510817                       [wonder, catch, flu, flight]   \n511019                       [never, felt, sick, stomach]   \n511481  [update, nearly, month, deathly, sick, finally...   \n511579                                      [cold, worth]   \n511604  [someone, buy, ill, able, buy, posca, pen, ill...   \n\n                                                     text               loc  \n50                                      sick of everyone.       sur califas  \n269                                   i have a headache ðŸ¤•        california  \n1034    at the vet with my 18 y/o rescue. has a cough....    california usa  \n1384                            iâ€™m dying my stomach done           omashu   \n1406    the headache this morning is on a whole other ...         deeetroit  \n...                                                   ...               ...  \n510817  @editingemily and wonder if you ll catch the f...  san francisco ca  \n511019                   never felt so sick to my stomach          working   \n511481  update: after nearly a month being deathly sic...       amarillo tx  \n511579           @thatiskkapri itâ€™s too cold not worth it        atlanta ga  \n511604  if someone buys this ill be able to buy more p...    california usa  \n\n[835 rows x 3 columns]\n['sur califas' 'california' 'california usa' 'omashu ' 'deeetroit'\n 'southwest atlanta ' 'houston' 'mars' 'paradise' 'chicago il'\n 'fort worth tx' 'cleveland heights oh' 'michigan usa' 'round rock tx'\n 'bronx ny' 'charlotte nc' 'ontario canada' 'providence ut' 'ferrell nj'\n 'connecticut usa' 'wiley college' 'beyhive' 'the denton high school'\n 'south richmond hill queens' 'missouri usa' 'atlanta ga' 'she her'\n 'san antonio tx' 'austin tx' 'new york usa' 'atlanta' 'kb ' 'dallas tx'\n 'reno nv' 'northeast florida' 'fresno ca' 'w my man eating lunch '\n 'smyrna ga' 'louisiana usa' 'nsm' 'mink rugs everywhere' 'alabama usa'\n 'raleigh nc' 'ohio' 'retroville' 'north carolina usa' 'at the gym' 'htx '\n 'columbia sc' 'moranjortsville aka stl ' 'pap mtl to barb s' 'houston '\n 'compton ca' 'htx' 'boston ma' 'north las vegas nv' 'peoria arizona '\n 'jackson ms' 'port arthur tx' 'seonghwa s kitchen ' 'columbus oh'\n 'los angeles ca' 'salt of the earth' 'stratford ct' 'omaha ne'\n 'paradise ' 'ya daddy house' 'tampa fl' 'garfield heights oh'\n 'queens brooklyn' 'new haven ct ' 'houston tx' 'brooklyn ny'\n 'baton rouge la' 'upper darby pa' 'the archangel valley ca'\n 'college park ga' 'toronto ontario' 'fl' 'new york ny' 'tulare ca'\n 'florida usa' 'alabama' 'virginia' 'virginia usa' 'keyport wa'\n 'easley sc' 'wild wild west ' 'toronto canada' 'pine grove' 'sj'\n 'st louis missouri' 'canal winchester oh' 'ku' 'san diego ca'\n 'myrtle beach sc' 'deshler ohio' 'texas usa' 'philly wherever i land'\n 'frederick maryalnd' 'monroe la' 'va' 'west covina ontario ca'\n 'durham nc' 'philadelphia pa' 'reality ' 'newark de' 'philadelph '\n 'prolly w bai los ' 'wherever' 'united states' 'hiding'\n ' so rad so real ' 'the city of glass ' 'living ' 'cole world ' 'ohio '\n 'wherever my girlfriend is ' 'duncanville tx' 'pittsburgh pa' 'carson ca'\n 'philadelphia pa ' 'las vegas nv' 'arizona usa' 'alief' 'krum tx'\n 'detroit mi' 'liberia' 'oklahoma us' 'richmond va' 'davenport ia'\n 'onde me la chupas' ' in my own world ' 'sc' 'east konoha' 'oh i o'\n 'somewhere between ssi and food stamps' 'torrance ca' 'clovis ca'\n 'felton de' 'ewayy' 'dirty dayton dusty akron'\n 'south los angeles los angeles' 'jacksonville fl '\n 'embracing your bitch ' 'seattle washington' 'alief houston beaumont'\n 'cary nc' 'mississippi usa' 'dallas tx canyon tx' 'long island ny '\n 'denver co' 'chicago il ' 'long beach ca' 'fort walton beach fl'\n 'nashville tn' 'sumner county' 'phl usa' 'met boy' 'bellevue oh'\n 'south carolina usa' 'sleep ' 'tdot' ' ' 'north texas atx' 'new orleans'\n 'grapes of wrath va' 'bay area ' 'phoenix az' 'washington usa'\n 'with no cap' 'under these bitches skin ' 'san francisco ca'\n 'cleveland oh' 'tallulah la' 'hayward ca' 'san jose ca' 'orlando fl'\n 'dreamville ' 'nm ' 'oklahoma texas ' 'new orleans la' 'duway '\n 'minding mines ' 'oxnard ca' 'memphi ' 'sacramento ca'\n 'san bernardino california' 'bronx ny brooklyn' 'astroworld '\n 'niceville fl' 'spring tx' 'boulder co' 'irmo sc' 'rural hall' 'tempe az'\n 'hiram clarke' 'notre dame united states' 'up all night'\n 'santa clarita ca' 'columbiana oh' 'texas' 'carson ca ' 'bakersfield ca'\n 'motunui' 'inglewood ca' 'world famous niagara falls' 'bellevue wa'\n 'east los angeles ca' 'milwaukee wi' 'hull ma' 'so cal' 'san fernando '\n 'palmdale' 'detroit michigan' 'newport news va' 'locked in '\n 'little rock ar' 'sa tx' 'pennsylvania usa' 'pensacola fl'\n 'washington state' 'destrehan la' 'hell' 'toronto'\n 'on to a new adventure ' 'georgia usa' 'ridgeway south carolina'\n 'alamitos beach long beach ca' 'london england' 'middletown ct'\n 'malvern oh' 'st catherine jamaica ' 'pearsall tx' 'honestly nowhere '\n 'atx' 'west deptford nj' 'iowa city ia' 'baton rouge' 'tennessee usa'\n 'dublin va usa' 'earlaz' 'independence mo' 'san luis obispo ca'\n 'lansing mi' 'charlit ' 'epcot' 'sometimes ny sometimes co'\n 'near augusta ga' 'ohira mura miyagi' 'flossy elmont'\n 'lexington kentucky' 'salt hate city ut' 'los angeles' 'chula vista ca'\n 'new london ct' 'with joc' 'tulsa ok' 'new hampshire usa' 'indianola ms'\n ' kyoto japan rosehill' 'everywhere ' 'happy valley utah' 'duckin lames'\n 'birnin zana wakanda' 'union city ca ' 'youngstown ohio' 'florida'\n 'minnesota usa' 'buffalo ny' 'lakewood oh' 'decatur il' 'chicago'\n 'i m getting there ' 'visalia ca' 'da trap da spot or da stu'\n 'rio grand valley nm' 'running a marathon ' 'swedesboro nj'\n 'here and there ' 'salt lake city ut' 'sanger ca'\n 'training facilty near you ' 'mobile al' 'neptune beach fl'\n 'christian county ky' 'chicago dallas texas usa' 'fucking texas'\n 'indio ca' 'az' 'in da hood ' 'washington dc' 'ypsilanti mi' 'norfolk va'\n 'bizness' 'somewhere not checking for you' 'murda worth' 'louisville ky'\n 'paradise city tx' 'marietta ga' 'cryin in the club'\n 'columbus ms milky way galaxy ' 'catskills ny' ' astroworld '\n 'eagle nebula' 'buckingham england' 'da boot' 'yulee fl' 'birmingham al'\n 'el salvador kansascity' 'columbus ga' 'indiana usa' 'locating '\n 'fraser michigan ' 'somewhere near you ' 'somewhere making moves'\n 'colorado' 'in the soul stone ' 'u s army' 'lemoore ca' 'matrix'\n 'baltimore ' 'dc' 'satx' 'above the rim' 'midwest' 'leamington ontario'\n 'living the sweet life ' 'toledo oh' 'st petersburg fl' 'everywhere'\n 'houston mf texas hoe she her'\n 'dallas texas via busta rhymes island massachusetts'\n 'middle of nowhere ohio' 'bay area california ' 'wonderland of madness'\n 'lake st louis mo' 'ny in' 'phoenix az ' 'opelousas la'\n 'leimert park los angeles' 'lower east side the bronx'\n 'virginia beach va' 'county ' 'belton tx' 'los angeles sf space'\n 'por ah ' 'in your heart' 'summerlin south nv' 'santa ana ca'\n 'san jose california' 'the streets where i belong' 'van nuys los angeles'\n 'ladera heights ca' 'idk' 'hattiesburg mississippi' 'vincennes in'\n 'grambling la' 'fort mill sc' 'washington' 'cedar rapids ia'\n 'new paris in' 'soutside lansing' 'lowell ma' 'akron oh' 'maryland'\n 'de soto mo' 'willoughby oh' 'houston texas' 'bartlesville ok'\n 'tuscaloosa al' 'mindin my business' 'urbana oh' 'oceanside ca'\n 'union city nj' 'ohio usa' 'new jersey usa' 'lapeer mi'\n 'oklahoma city ok' 'pickerington oh' 'jacksonville beach fl' 'algiers la'\n 'oklahoma' 'd m v morgantown' 'wilmington manor de' 'dallas'\n 'chasing the golden hour ' 'west by god' 'blue springs mo'\n 'grand rapids mi' 'maryland usa' 'humble tx' 'battle creek mi' 'la'\n 'mo usa' 'arbuckle mountains ' ' sun moon rising' 'nowhere '\n 'fort wayne in' 'shoreview ca' 'dayton oh' 'guam' 'starkvegas'\n 'indianapolis in' 'chandler az' 'toledo ohio' 'bay area' 'chico ca'\n 'khi nyc yyz' 'dmv' 'under the electric sky' 'knockin on heavens door '\n 'la sf' 'columbus oh ' 'clarksburg wv' 'indialantic fl' 'milpitas ca'\n 'cali zona' 'big spring tx' 'twitch' 'bay area ca '\n 'atlanta ga sacramento ca' 'san marcos san antonio tx' 'manhattan ny'\n 'montgomery al ' 'santa paula' 'lancaster ca' 'morrilton ar'\n 'a strange laboratory' 'new orleans louisiana' 'el centro ca'\n 'in the gym and garden' 'greensboro nc for now ' 'dallas ' 'lil mexico'\n 'philly' 'yucaipa ca' 'ky tn' 'carthage ar ' 'urbana il' 'love dump '\n 'wrightsville ga' 'arlington tx' 'somewhere w jm riley fishing'\n 'perris ca' 'cecil county maryland' 'northern wi ' 'cedar hill tx'\n 'colorado ' 'buffalo ny dallas houston tx ' 'montreal' 'savannah ga'\n 'somewhere eating in atlanta ' 'oklahoma usa' 'new york new york ' 'ct'\n 'the wind just blew me here' 'where we say eh ' 'shafergerry gmail com'\n 'huntsville al' 'cecilia lsu ' 'heaven' 'dumas ar' 'talladega al'\n 'back streets' 'ashburn ga ' 'atlanta ga but tupelo is home' 'my garden '\n 'pr fl' 'greenville tx' 'st charles mo' 'riverside ca'\n 'talking to animals ' 'nj atl' 'planet vegeta' 'wisco' 'el monte ca'\n 'minneapolis mn' 'phx ' 'aokigahara japan' 'milwaukee wisconsin'\n 'oklahoma ' 'west hollywood california' 'ukiah ca'\n 'baltimore los angeles' 'chicago illannoys' 'jefferson city mo'\n 'wickliffe oh' 'oakland ca' 'influencer' 'new orleans ' 'lost'\n 'reading pa' 'west hollywood ca' 'canada' 'wake forest nc' 'the o'\n 'east greenwich ri' 'dtx' 'ph nv tx' 'hogwarts school '\n 'in the quiet of my mind' 'houston texas ' 'atl' 'corpus christi tx'\n 'the boot' 'pearland tx' 'ft worth stephenville tx' 'spartanburg sc'\n 'wepo ' 'new mexico usa' 'silver springs florida' 'fort lauderdale fl'\n 'denton tx' 'santa barbara ca' 'llj ' 'virginia houston' 'moore ok'\n 'mcallen d c' 'suwanee ga' ' verified austell ga'\n 'baton rouge la grambling' 'etx la' 'phoenix arizona' 'my shawy s krib'\n 'your girls house' 'hiram clarke austin' 'chelsea manhattan' 'nyc'\n 'richmond ca' 'miami ok' 'fla va atl orlando ' 'mind of a werid guy '\n 'minding my own ' 'seattle wa ' 'probably at work ' 'isla vista ca'\n 'dearborn mi' 'jenny chen s america' 'long beach ca ' 'norcal'\n 'orange va' 'peace' 'calif rnia' 'cincinnati' 'new jersey' 'tucson az'\n 'shoplifting at whole foods ' 'baseworld' 'anime witch girl world'\n 'fontana ca' 'ny atl la ' 'out south chicago' 'utah usa' 'working '\n 'amarillo tx']\n"
    }
   ],
   "source": [
    "idx = np.where(similarities > .15)\n",
    "print(tweets.iloc[idx][['processed_text', 'text', 'loc']])\n",
    "print(tweets.iloc[idx]['loc'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "831 tweets are somewhat similiar/contain some of our keywords in question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final step (?): identify actual geolocation entities within the location\n",
    "disease_tweets = tweets.iloc[idx]\n",
    "#for index, t in disease_tweets.iterrows():\n",
    "#    places = GeoText(t['loc'].title()) # first letter needs to be uppercase\n",
    "#    disease_tweets = disease_tweets.append({'real_loc':places.cities if len(places.cities) > 0 else ''}, ignore_index = True)\n",
    "    #tweets.iloc[index]['real_loc'] = places.cities if len(places.cities) > 0 else None\n",
    "    #print(places.country_mentions)\n",
    "#print(GeoText('new York').cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>loc</th>\n      <th>text</th>\n      <th>processed_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>50</th>\n      <td>sur califas</td>\n      <td>sick of everyone.</td>\n      <td>[sick, everyone]</td>\n    </tr>\n    <tr>\n      <th>269</th>\n      <td>california</td>\n      <td>i have a headache ðŸ¤•</td>\n      <td>[headache]</td>\n    </tr>\n    <tr>\n      <th>1034</th>\n      <td>california usa</td>\n      <td>at the vet with my 18 y/o rescue. has a cough....</td>\n      <td>[vet, rescue, cough, think, kennel, cough, go,...</td>\n    </tr>\n    <tr>\n      <th>1384</th>\n      <td>omashu</td>\n      <td>iâ€™m dying my stomach done</td>\n      <td>[die, stomach, do]</td>\n    </tr>\n    <tr>\n      <th>1406</th>\n      <td>deeetroit</td>\n      <td>the headache this morning is on a whole other ...</td>\n      <td>[headache, morning, whole, level]</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>510817</th>\n      <td>san francisco ca</td>\n      <td>@editingemily and wonder if you ll catch the f...</td>\n      <td>[wonder, catch, flu, flight]</td>\n    </tr>\n    <tr>\n      <th>511019</th>\n      <td>working</td>\n      <td>never felt so sick to my stomach</td>\n      <td>[never, felt, sick, stomach]</td>\n    </tr>\n    <tr>\n      <th>511481</th>\n      <td>amarillo tx</td>\n      <td>update: after nearly a month being deathly sic...</td>\n      <td>[update, nearly, month, deathly, sick, finally...</td>\n    </tr>\n    <tr>\n      <th>511579</th>\n      <td>atlanta ga</td>\n      <td>@thatiskkapri itâ€™s too cold not worth it</td>\n      <td>[cold, worth]</td>\n    </tr>\n    <tr>\n      <th>511604</th>\n      <td>california usa</td>\n      <td>if someone buys this ill be able to buy more p...</td>\n      <td>[someone, buy, ill, able, buy, posca, pen, ill...</td>\n    </tr>\n  </tbody>\n</table>\n<p>835 rows Ã— 3 columns</p>\n</div>",
      "text/plain": "                     loc                                               text  \\\n50           sur califas                                  sick of everyone.   \n269           california                                i have a headache ðŸ¤•   \n1034      california usa  at the vet with my 18 y/o rescue. has a cough....   \n1384             omashu                           iâ€™m dying my stomach done   \n1406           deeetroit  the headache this morning is on a whole other ...   \n...                  ...                                                ...   \n510817  san francisco ca  @editingemily and wonder if you ll catch the f...   \n511019          working                    never felt so sick to my stomach   \n511481       amarillo tx  update: after nearly a month being deathly sic...   \n511579        atlanta ga           @thatiskkapri itâ€™s too cold not worth it   \n511604    california usa  if someone buys this ill be able to buy more p...   \n\n                                           processed_text  \n50                                       [sick, everyone]  \n269                                            [headache]  \n1034    [vet, rescue, cough, think, kennel, cough, go,...  \n1384                                   [die, stomach, do]  \n1406                    [headache, morning, whole, level]  \n...                                                   ...  \n510817                       [wonder, catch, flu, flight]  \n511019                       [never, felt, sick, stomach]  \n511481  [update, nearly, month, deathly, sick, finally...  \n511579                                      [cold, worth]  \n511604  [someone, buy, ill, able, buy, posca, pen, ill...  \n\n[835 rows x 3 columns]"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disease_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datetime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-cdc93c7a94ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m         ).add_to(tweets_map)\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mtweets_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tweets_and_locations_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromtimestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%Y-%m-%d_%H:%M:%S'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.html'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mtweets_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datetime' is not defined"
     ]
    }
   ],
   "source": [
    "# Now let's plot the tweets together with it's location on a map\n",
    "latitude = 37.0902 # USA\n",
    "longitude = -95.7129 # USA\n",
    "\n",
    "tweets_map = folium.Map(location=[latitude, longitude], zoom_start=5)\n",
    "for i, t in disease_tweets.iterrows():\n",
    "    # check if a real location name can be inferred\n",
    "    places = GeoText(t['loc'].title())\n",
    "    if(len(places.cities) > 0):\n",
    "        gn = geocoder.geonames(places.cities[0], key = 'lefkokills') # access geonames webservice\n",
    "        folium.CircleMarker(\n",
    "            [gn.lat, gn.lng],\n",
    "            radius=1.5,\n",
    "            popup = ('City: ' + places.cities[0] + '<br>'\n",
    "                'Tweet: ' + t['text']\n",
    "                ),\n",
    "        color=\"#007849\",\n",
    "        #key_on = traffic_q,\n",
    "        #threshold_scale=[0,1,2,3],\n",
    "        #fill_color=colordict[traffic_q],\n",
    "        fill=True,\n",
    "        fill_opacity=0.7\n",
    "        ).add_to(tweets_map)\n",
    "\n",
    "tweets_map.save('tweets_and_locations_' + datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d_%H:%M:%S') + '.html')    \n",
    "tweets_map"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## STOP HERE, BACKUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cache\n",
    "'''\n",
    "With the need to do text clustering at sentence level there will be one extra step for moving from word level to sentence level. For each sentence from the set of sentences, word embedding of each word is summed and in the end divided by number of words in the sentence. So we are getting average of all word embeddings for each sentence and use them as we would use embeddings at word level\n",
    "'''\n",
    "def sent_vectorizer(sent, model):\n",
    "    sent_vec =[]\n",
    "    numw = 0\n",
    "    for w in sent:\n",
    "        try:\n",
    "            if numw == 0:\n",
    "                sent_vec = model[w]\n",
    "            else:\n",
    "                sent_vec = np.add(sent_vec, model[w])\n",
    "            numw+=1\n",
    "        except:\n",
    "            pass\n",
    "     \n",
    "    return np.asarray(sent_vec) / numw\n",
    "  \n",
    "  \n",
    "X=[]\n",
    "for pre_processed_tweet in tweets['processed_text']:\n",
    "    X.append(sent_vectorizer(pre_processed_tweet, w2vmodel))\n",
    "\n",
    "print(len(X), 'sentences are vectorised')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tweet Clustering (not yet done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cache\n",
    "def printLabels(labels, model):\n",
    "    words = list(model.wv.vocab)\n",
    "    for i, word in enumerate(words):  \n",
    "        print(word + \":\" + str(labels[i]))\n",
    "\n",
    "km = KMeans(n_clusters=50, init='k-means++', max_iter=100, n_init=1, verbose=True)\n",
    "\n",
    "print(\"Clustering sparse data with %s\" % km)\n",
    "%time km.fit(tweet_vecs)\n",
    "\n",
    "labels = km.labels_ # assigned labels\n",
    "centroids = km.cluster_centers_ # cluster centroids\n",
    "#printLabels(labels, w2vmodel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A simpler approach (backup plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cache\n",
    "%%time\n",
    "# use fuzzy-matching to look for tweets containing these words (or similar ones)\n",
    "# https://www.datacamp.com/community/tutorials/fuzzy-string-python\n",
    "def simRatio(tweet_token, keyword):\n",
    "    '''\n",
    "        calculates the similarity between a token and a keyword\n",
    "    '''\n",
    "    return fuzz.ratio(tweet_token, keyword)\n",
    "\n",
    "from operator import itemgetter\n",
    "def max_val(l, i):\n",
    "    return max(enumerate(map(itemgetter(i), l)),key=itemgetter(1))\n",
    "\n",
    "ratios_per_tweet = pd.DataFrame(None, columns=['id', 'ratio_mean', 'ratio_clean', 'ratio_median', 'tokens'])\n",
    "id = 0\n",
    "\n",
    "for tweet_tokens in tweets['processed_text']:\n",
    "    ratio = [simRatio(token, keyword) for token in tweet_tokens for keyword in kws]\n",
    "    if(np.mean(ratio) > 25):\n",
    "        #tweet_ids.append(id)\n",
    "        ratios_per_tweet = ratios_per_tweet.append({'id':id, 'ratio_mean':np.mean(ratio), 'ratio_clean':np.asarray(ratio)/len(tweet_tokens), 'ratio_median':np.median(ratio), 'tokens':tweet_tokens}, ignore_index=True)\n",
    "\n",
    "    id +=1\n",
    "\n",
    "print(len(ratios_per_tweet), 'tweets found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cache\n",
    "ratios_per_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cache\n",
    "ratios_per_tweet[ratios_per_tweet['ratio_mean'] > 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}