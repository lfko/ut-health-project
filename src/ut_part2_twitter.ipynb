{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464bitlficonda5c33be88125242aca6d772c29aa3d2b6",
   "display_name": "Python 3.7.4 64-bit ('lfi': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "[nltk_data] Downloading package punkt to /home/lefko/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /home/lefko/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /home/lefko/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Folium\n",
    "import folium \n",
    "\n",
    "# geotext\n",
    "from geotext import GeoText\n",
    "import geocoder\n",
    "\n",
    "# SciPy\n",
    "from scipy import stats\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# fuzzywuzzy\n",
    "from fuzzywuzzy import fuzz # install python-Levenshtein afterwards for speeding up things\n",
    "\n",
    "# gensim\n",
    "from gensim.models import Word2Vec, TfidfModel\n",
    "from gensim import corpora, models\n",
    "from gensim.similarities import Similarity\n",
    "\n",
    "# sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global properties\n",
    "workDir = '/home/lefko/git/ut-health-project'\n",
    "dbfile = workDir + '/db/tweets.csv'\n",
    "states = workDir + '/data/us-states-abbr.csv'\n",
    "\n",
    "_stopwords = ['AT_USER', 'URL'] + stopwords.words('english')\n",
    "_lem = WordNetLemmatizer()\n",
    "sample = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the tweets file can be accessed\n",
    "try:\n",
    "    f = open(dbfile)\n",
    "except IOError:\n",
    "    print(\"File not accessible\")\n",
    "finally:\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    some utility function\n",
    "'''\n",
    "def readCitiesFromFile(state):\n",
    "    cities = []\n",
    "    with open(workDir + '/data/cities_' + state + '.txt') as txt:\n",
    "        for line in txt:\n",
    "            cities.append(line.strip().lower())\n",
    "\n",
    "    return pd.DataFrame(cities, columns = ['city'])\n",
    "\n",
    "def readKeywords():\n",
    "    keywords = []\n",
    "\n",
    "    with open(workDir + '/data/keywords.txt') as txt:\n",
    "        for line in txt:\n",
    "            keywords.append(line.strip().lower())\n",
    "\n",
    "    print(keywords)\n",
    "    return keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_colorado = readCitiesFromFile('co') # Colorado cities\n",
    "cities_california = readCitiesFromFile('ca') # California cities\n",
    "cities_massachussetts = readCitiesFromFile('ma') # Massachussetts cities\n",
    "cities_all = cities_california + cities_colorado + cities_massachussetts # all 3 of them combined\n",
    "\n",
    "states_abbr = pd.read_csv(states, sep=',') # official abbreviations for the states\n",
    "states_abbr = states_abbr.applymap(lambda s:s.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['AT_USER', 'URL', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'al', 'ak', 'az', 'ar', 'ca', 'co', 'ct', 'de', 'fl', 'ga', 'hi', 'id', 'il', 'in', 'ia', 'ks', 'ky', 'la', 'me', 'md', 'ma', 'mi', 'mn', 'ms', 'mo', 'mt', 'ne', 'nv', 'nh', 'nj', 'nm', 'ny', 'nc', 'nd', 'oh', 'ok', 'or', 'pa', 'ri', 'sc', 'sd', 'tn', 'tx', 'usa', 'ut', 'vt', 'va', 'wa', 'wv', 'wi', 'wy']\n"
    }
   ],
   "source": [
    "abbrs = []\n",
    "for i in range(len(states_abbr)):\n",
    "    abbrs.append(states_abbr.iloc[:, 1][i].strip())\n",
    "    #abbrs.append(states_abbr.iloc[:, 1][i])\n",
    "\n",
    "_stopwords += abbrs # add the abbreviations to the list of stopwords aswell\n",
    "print(_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "CPU times: user 972 ms, sys: 74.1 ms, total: 1.05 s\nWall time: 929 ms\ntweets dataframe created!\nCPU times: user 608 ms, sys: 27.8 ms, total: 635 ms\nWall time: 635 ms\nall strings are lowered\n"
    }
   ],
   "source": [
    "%time tweets = pd.read_csv(dbfile, names=['tweet_id', 'created', 'loc', 'text'], header=None) # create a new DataFrame, which holds the tweets\n",
    "\n",
    "if(sample > 0):\n",
    "    tweets = tweets[0:sample]\n",
    "\n",
    "print('tweets dataframe created!')\n",
    "tweets = tweets[tweets['loc'].isnull() == False] # sort out NaN places\n",
    "tweets = tweets[tweets['loc'].str.contains('\\d') == False] # sort out tweets where the location contains numbers\n",
    "tweets = tweets.drop(['tweet_id', 'created'], axis=1) # they are not needed, actually\n",
    "%time tweets = tweets.applymap(lambda s:s.lower() if type(s) == str else s) # lower all strings\n",
    "print('all strings are lowered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "414116\n"
    }
   ],
   "source": [
    "# How many tweets are there in total now?\n",
    "print(len(tweets))\n",
    "sample = len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "39810 unique locations found\n76.74311062600817 average length of a tweet\n"
    }
   ],
   "source": [
    "# How many unique locations?\n",
    "print(len(tweets['loc'].unique()), 'unique locations found')\n",
    "# Average tweet length?\n",
    "print(np.mean(tweets['text'].str.len()), 'average length of a tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>loc</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>502314</th>\n      <td>texas usa</td>\n      <td>thats my girl.</td>\n    </tr>\n    <tr>\n      <th>502315</th>\n      <td>arizona usa</td>\n      <td>dick up that latte art ü§ôüèº</td>\n    </tr>\n    <tr>\n      <th>502316</th>\n      <td>liberal america s extreme left</td>\n      <td>well deserved!! thank you mr &amp;amp; mrs cooper ...</td>\n    </tr>\n    <tr>\n      <th>502317</th>\n      <td>norfolk va</td>\n      <td>i just be feeling stuck ..</td>\n    </tr>\n    <tr>\n      <th>502318</th>\n      <td>roseburg or</td>\n      <td>@cum4perkytits lets fuck</td>\n    </tr>\n    <tr>\n      <th>502319</th>\n      <td>nova</td>\n      <td>work didn't close, no cave for me tonight.\\n\\n...</td>\n    </tr>\n    <tr>\n      <th>502321</th>\n      <td>oklahoma usa</td>\n      <td>@jkerr190 @iwf @missdiagnosis @foxnews @foxfri...</td>\n    </tr>\n    <tr>\n      <th>502322</th>\n      <td>norfolk va</td>\n      <td>i‚Äôm sorry but how is tonight‚Äôs sky even real?üòÖ...</td>\n    </tr>\n    <tr>\n      <th>502323</th>\n      <td>verdigris ok</td>\n      <td>@pulliamtaylork you didn‚Äôt make plans with me</td>\n    </tr>\n    <tr>\n      <th>502324</th>\n      <td>aliso viejo ca</td>\n      <td>parents please join us over coffee and cookies...</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                                   loc  \\\n502314                       texas usa   \n502315                     arizona usa   \n502316  liberal america s extreme left   \n502317                      norfolk va   \n502318                     roseburg or   \n502319                            nova   \n502321                    oklahoma usa   \n502322                     norfolk va    \n502323                    verdigris ok   \n502324                  aliso viejo ca   \n\n                                                     text  \n502314                                     thats my girl.  \n502315                          dick up that latte art ü§ôüèº  \n502316  well deserved!! thank you mr &amp; mrs cooper ...  \n502317                         i just be feeling stuck ..  \n502318                           @cum4perkytits lets fuck  \n502319  work didn't close, no cave for me tonight.\\n\\n...  \n502321  @jkerr190 @iwf @missdiagnosis @foxnews @foxfri...  \n502322  i‚Äôm sorry but how is tonight‚Äôs sky even real?üòÖ...  \n502323      @pulliamtaylork you didn‚Äôt make plans with me  \n502324  parents please join us over coffee and cookies...  "
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the 10 latest tweets\n",
    "tweets.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We already see a problem here: locations are not always real or useful at all. We'll have to deal with this accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NLP Tweet processing\n",
    "### Tokenization, Stopwords and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "CPU times: user 2min 27s, sys: 255 ms, total: 2min 27s\nWall time: 2min 27s\n"
    }
   ],
   "source": [
    "%%time\n",
    "# tweet tokenization and stopword removal\n",
    "def processTweet(tweet_text):\n",
    "    tweet_text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', 'URL', tweet_text) # remove URLs\n",
    "    tweet_text = re.sub('@[^\\s]+', 'AT_USER', tweet_text) # remove usernames\n",
    "    tweet_text = re.sub(r'#([^\\s]+)', r'\\1', tweet_text) # remove the # in #hashtag\n",
    "    tweet_text = [word for word in word_tokenize(tweet_text) if word.isalpha() == True and word not in _stopwords] # tokenize the text\n",
    "    tweet_text = [_lem.lemmatize(word,'v') for word in tweet_text] # lemmatize\n",
    "    return tweet_text\n",
    "\n",
    "tweets['processed_text']=''\n",
    "processed_tweets = []\n",
    "for i in range(sample):\n",
    "    tweet = tweets.iloc[i]\n",
    "    processed_tweets.append(processTweet(tweet['text']))\n",
    "    #tweet.iloc[i]['processed_text'] = processTweet(tweet['text'])\n",
    "\n",
    "tweets['processed_text'][0:sample] = processed_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "CPU times: user 419 ¬µs, sys: 0 ns, total: 419 ¬µs\nWall time: 405 ¬µs\n"
    },
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>loc</th>\n      <th>text</th>\n      <th>processed_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>all over the place</td>\n      <td>@jordanmastagni3 \\r\\n\\r\\nthank you for the fol...</td>\n      <td>[thank, follow, jordan]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>los angeles ca</td>\n      <td>can you recommend anyone for this #skilledtrad...</td>\n      <td>[recommend, anyone, skilledtrade, job, palmdal...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>redding ca</td>\n      <td>rehearsals have been going well. we‚Äôre set to ...</td>\n      <td>[rehearsals, go, well, set, film, rest, new, m...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>pine and buckeye az</td>\n      <td>2013 please read it. let's media spread their ...</td>\n      <td>[please, read, let, media, spread, lie, well, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>montclair ca</td>\n      <td>@gennefer @davidgrosstv very cool!!!!</td>\n      <td>[cool]</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>hanford ca</td>\n      <td>@mholder95 i thought it was hilarious. if you ...</td>\n      <td>[think, hilarious, laugh, lose, mind, humor, s...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>hollister ca</td>\n      <td>these days i balance all the hate out with the...</td>\n      <td>[days, balance, hate, love]</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>marina del rey ca</td>\n      <td>works out good for the canes since we just got...</td>\n      <td>[work, good, can, since, get, beat, bobby, bou...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>palm springs calif</td>\n      <td>impeach nancy?</td>\n      <td>[impeach, nancy]</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>laguna beach ca</td>\n      <td>getting a little love jeanninesrestaurants whi...</td>\n      <td>[get, little, love, jeanninesrestaurants, spur...</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                   loc                                               text  \\\n0   all over the place  @jordanmastagni3 \\r\\n\\r\\nthank you for the fol...   \n1       los angeles ca  can you recommend anyone for this #skilledtrad...   \n2           redding ca  rehearsals have been going well. we‚Äôre set to ...   \n3  pine and buckeye az  2013 please read it. let's media spread their ...   \n4         montclair ca              @gennefer @davidgrosstv very cool!!!!   \n5           hanford ca  @mholder95 i thought it was hilarious. if you ...   \n6         hollister ca  these days i balance all the hate out with the...   \n7    marina del rey ca  works out good for the canes since we just got...   \n8   palm springs calif                                     impeach nancy?   \n9      laguna beach ca  getting a little love jeanninesrestaurants whi...   \n\n                                      processed_text  \n0                            [thank, follow, jordan]  \n1  [recommend, anyone, skilledtrade, job, palmdal...  \n2  [rehearsals, go, well, set, film, rest, new, m...  \n3  [please, read, let, media, spread, lie, well, ...  \n4                                             [cool]  \n5  [think, hilarious, laugh, lose, mind, humor, s...  \n6                        [days, balance, hate, love]  \n7  [work, good, can, since, get, beat, bobby, bou...  \n8                                   [impeach, nancy]  \n9  [get, little, love, jeanninesrestaurants, spur...  "
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time tweets.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Vectorization\n",
    "Let's represent our tweets as numerical values, i.e. vectors in a vector space\n",
    "\n",
    "Note that, a ‚Äòtoken‚Äô typically means a ‚Äòword‚Äô. A ‚Äòdocument‚Äô can typically refer to a ‚Äòsentence‚Äô or ‚Äòparagraph‚Äô and a ‚Äòcorpus‚Äô is typically a ‚Äòcollection of documents as a bag of words‚Äô.\n",
    "\n",
    "https://dev.to/coderasha/compare-documents-similarity-using-python-nlp-4odp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(414116, 90967)\nCPU times: user 1.92 s, sys: 16 ms, total: 1.94 s\nWall time: 1.95 s\n"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# https://stackoverflow.com/questions/35867484/pass-tokens-to-countvectorizer\n",
    "def usePreprocessed(doc):\n",
    "    return doc\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=usePreprocessed,preprocessor=None, lowercase=False) # CV does all the tokenization and stopword removal all by itself already\n",
    "#tfidf = TfidfVectorizer()\n",
    "tweet_vecs = vectorizer.fit_transform(tweets['processed_text']) # sparse matrix\n",
    "#print(vectorizer.get_feature_names())\n",
    "#print(tweet_vecs.toarray())\n",
    "print(tweet_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "CPU times: user 8.24 s, sys: 96.3 ms, total: 8.34 s\nWall time: 8.6 s\n"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "'''\n",
    "In order to work on text documents, Gensim requires the words (aka tokens) be converted to unique ids. So, Gensim lets you create a Dictionary object that maps each word to a unique id. Let's convert our sentences to a [list of words] and pass it to the corpora.Dictionary() object.\n",
    "'''\n",
    "corpDict = corpora.Dictionary(tweets['processed_text'])\n",
    "corpus = [corpDict.doc2bow(text) for text in tweets['processed_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Corpus is a Bag of Words. It is a basically object that contains the word id and its frequency in each document (just lists the number of times each word occurs in the sentence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Term Frequency ‚Äì Inverse Document Frequency(TF-IDF) is also a bag-of-words model but unlike the regular corpus, TFIDF down weights tokens (words) that appears frequently across documents.\n",
    "\n",
    "Tf-Idf is calculated by multiplying a local component (TF) with a global component (IDF) and optionally normalizing the result to unit length. Term frequency is how often the word shows up in the document and inverse document frequency scales the value by how rare the word is in the corpus. In simple terms, words that occur more frequently across the documents get smaller weights.\n",
    "'''\n",
    "tf_idf = TfidfModel(corpus)\n",
    "#for doc in tf_idf[corpus]:\n",
    "#    print([[corpDict[id], np.around(freq, decimals=2)] for id, freq in doc])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup Similarity measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['ill', 'sick', 'cold', 'flu', 'influenza', 'disease', 'weak', 'cough', 'headache', 'stomach']\n10 keywords loaded!\n"
    }
   ],
   "source": [
    "# load some illness-related keywords\n",
    "kws = readKeywords()\n",
    "print(len(kws), 'keywords loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './ut-health-project/data/.0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/lfi/lib/python3.7/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fname_or_handle, separately, sep_limit, ignore, pickle_protocol)\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m             \u001b[0m_pickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"saved %s object\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: file must have a 'write' attribute",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-1c9ad8ddf616>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# We are storing index matrix in 'data' directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./ut-health-project/data/'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf_idf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpDict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# update the already existing dictionary with the keywords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mkws_bow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpDict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lfi/lib/python3.7/site-packages/gensim/similarities/docsim.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, output_prefix, corpus, num_features, num_best, chunksize, shardsize, norm)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lfi/lib/python3.7/site-packages/gensim/similarities/docsim.py\u001b[0m in \u001b[0;36madd_documents\u001b[0;34m(self, corpus)\u001b[0m\n\u001b[1;32m    388\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfresh_nnz\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdoclen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfresh_docs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshardsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_shard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfresh_docs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PROGRESS: fresh_shard size=%i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfresh_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lfi/lib/python3.7/site-packages/gensim/similarities/docsim.py\u001b[0m in \u001b[0;36mclose_shard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMatrixSimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfresh_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"creating %s shard #%s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sparse'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'dense'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshardid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m         \u001b[0mshard\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mShard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshardid2filename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshardid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m         \u001b[0mshard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_best\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_best\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0mshard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_nnz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfresh_nnz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lfi/lib/python3.7/site-packages/gensim/similarities/docsim.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fname, index)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"saving index shard to %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfullname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfullname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lfi/lib/python3.7/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fname_or_handle, separately, sep_limit, ignore, pickle_protocol)\u001b[0m\n\u001b[1;32m    693\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"saved %s object\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# `fname_or_handle` does not have write attribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_smart_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseparately\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep_limit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lfi/lib/python3.7/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36m_smart_save\u001b[0;34m(self, fname, separately, sep_limit, ignore, pickle_protocol)\u001b[0m\n\u001b[1;32m    547\u001b[0m                                        compress, subname)\n\u001b[1;32m    548\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m             \u001b[0mpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;31m# restore attribs handled specially\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lfi/lib/python3.7/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mpickle\u001b[0;34m(obj, fname, protocol)\u001b[0m\n\u001b[1;32m   1361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m     \"\"\"\n\u001b[0;32m-> 1363\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfout\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 'b' for binary, needed on Windows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1364\u001b[0m         \u001b[0m_pickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lfi/lib/python3.7/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, transport_params)\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m         \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m     )\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lfi/lib/python3.7/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, ignore_ext, buffering, encoding, errors)\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPY3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './ut-health-project/data/.0'"
     ]
    }
   ],
   "source": [
    "# We are storing index matrix in 'data' directory\n",
    "sims = Similarity('./ut-health-project/data/',tf_idf[corpus], num_features=len(corpDict))\n",
    "\n",
    "# update the already existing dictionary with the keywords\n",
    "kws_bow = corpDict.doc2bow(kws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# perform a similarity query against the corpus\n",
    "kws_bow_tf_idf = tf_idf[kws_bow]\n",
    "# print(document_number, document_similarity)\n",
    "#print('Comparing Result:', sims[kws_bow_tf_idf]) \n",
    "similarities = sims[kws_bow_tf_idf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.where(similarities > .15)\n",
    "print(tweets.iloc[idx][['processed_text', 'text', 'loc']])\n",
    "print(tweets.iloc[idx]['loc'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "821 tweets are somewhat similiar/contain some of our keywords in question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final step (?): identify actual geolocation entities within the location\n",
    "disease_tweets = tweets.iloc[idx]\n",
    "for index, t in disease_tweets.iterrows():\n",
    "    places = GeoText(t['loc'].title()) # first letter needs to be uppercase\n",
    "    disease_tweets = disease_tweets.append({'real_loc':places.cities if len(places.cities) > 0 else np.nan}, ignore_index = True)\n",
    "    tweets.iloc[index]['real_loc'] = places.cities if len(places.cities) > 0 else np.nan\n",
    "    #print(places.country_mentions)\n",
    "#print(GeoText('new York').cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = tweets[(tweets['real_loc'].isna() == False) & (tweets['text'].isna() == False)]\n",
    "print(plot_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's plot the tweets together with it's location on a map\n",
    "latitude = 37.0902 # USA\n",
    "longitude = -95.7129 # USA\n",
    "\n",
    "tweets_map = folium.Map(location=[latitude, longitude], zoom_start=5)\n",
    "for i, t in plot_data.iterrows()::\n",
    "    gn = geocoder.geonames('San Antonio', key = 'lefkokills') # access geonames webservice\n",
    "    folium.CircleMarker(\n",
    "        [gn.lat, gn.lng],\n",
    "        radius=.15,\n",
    "        popup = ('City: ' + t['real_loc'] + '<br>'\n",
    "            'Tweet: ' + t['text']\n",
    "            ),\n",
    "    color='b',\n",
    "    #key_on = traffic_q,\n",
    "    threshold_scale=[0,1,2,3],\n",
    "    #fill_color=colordict[traffic_q],\n",
    "    fill=True,\n",
    "    fill_opacity=0.7\n",
    "    ).add_to(tweets_map)\n",
    "    \n",
    "tweets_map"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## STOP HERE, BACKUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cache\n",
    "'''\n",
    "With the need to do text clustering at sentence level there will be one extra step for moving from word level to sentence level. For each sentence from the set of sentences, word embedding of each word is summed and in the end divided by number of words in the sentence. So we are getting average of all word embeddings for each sentence and use them as we would use embeddings at word level\n",
    "'''\n",
    "def sent_vectorizer(sent, model):\n",
    "    sent_vec =[]\n",
    "    numw = 0\n",
    "    for w in sent:\n",
    "        try:\n",
    "            if numw == 0:\n",
    "                sent_vec = model[w]\n",
    "            else:\n",
    "                sent_vec = np.add(sent_vec, model[w])\n",
    "            numw+=1\n",
    "        except:\n",
    "            pass\n",
    "     \n",
    "    return np.asarray(sent_vec) / numw\n",
    "  \n",
    "  \n",
    "X=[]\n",
    "for pre_processed_tweet in tweets['processed_text']:\n",
    "    X.append(sent_vectorizer(pre_processed_tweet, w2vmodel))\n",
    "\n",
    "print(len(X), 'sentences are vectorised')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tweet Clustering (not yet done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cache\n",
    "def printLabels(labels, model):\n",
    "    words = list(model.wv.vocab)\n",
    "    for i, word in enumerate(words):  \n",
    "        print(word + \":\" + str(labels[i]))\n",
    "\n",
    "km = KMeans(n_clusters=50, init='k-means++', max_iter=100, n_init=1, verbose=True)\n",
    "\n",
    "print(\"Clustering sparse data with %s\" % km)\n",
    "%time km.fit(tweet_vecs)\n",
    "\n",
    "labels = km.labels_ # assigned labels\n",
    "centroids = km.cluster_centers_ # cluster centroids\n",
    "#printLabels(labels, w2vmodel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A simpler approach (backup plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cache\n",
    "%%time\n",
    "# use fuzzy-matching to look for tweets containing these words (or similar ones)\n",
    "# https://www.datacamp.com/community/tutorials/fuzzy-string-python\n",
    "def simRatio(tweet_token, keyword):\n",
    "    '''\n",
    "        calculates the similarity between a token and a keyword\n",
    "    '''\n",
    "    return fuzz.ratio(tweet_token, keyword)\n",
    "\n",
    "from operator import itemgetter\n",
    "def max_val(l, i):\n",
    "    return max(enumerate(map(itemgetter(i), l)),key=itemgetter(1))\n",
    "\n",
    "ratios_per_tweet = pd.DataFrame(None, columns=['id', 'ratio_mean', 'ratio_clean', 'ratio_median', 'tokens'])\n",
    "id = 0\n",
    "\n",
    "for tweet_tokens in tweets['processed_text']:\n",
    "    ratio = [simRatio(token, keyword) for token in tweet_tokens for keyword in kws]\n",
    "    if(np.mean(ratio) > 25):\n",
    "        #tweet_ids.append(id)\n",
    "        ratios_per_tweet = ratios_per_tweet.append({'id':id, 'ratio_mean':np.mean(ratio), 'ratio_clean':np.asarray(ratio)/len(tweet_tokens), 'ratio_median':np.median(ratio), 'tokens':tweet_tokens}, ignore_index=True)\n",
    "\n",
    "    id +=1\n",
    "\n",
    "print(len(ratios_per_tweet), 'tweets found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cache\n",
    "ratios_per_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cache\n",
    "ratios_per_tweet[ratios_per_tweet['ratio_mean'] > 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}