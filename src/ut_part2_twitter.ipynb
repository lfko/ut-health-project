{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "[nltk_data] Error loading punkt: <urlopen error [Errno -3] Temporary\n[nltk_data]     failure in name resolution>\n[nltk_data] Error loading stopwords: <urlopen error [Errno -3]\n[nltk_data]     Temporary failure in name resolution>\n[nltk_data] Error loading wordnet: <urlopen error [Errno -3] Temporary\n[nltk_data]     failure in name resolution>\n"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "#from tinydb import TinyDB\n",
    "#import requests\n",
    "#from bs4 import BeautifulSoup\n",
    "#from urllib.request import urlopen\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "workDir = '/home/lefko/git/ut-health-project'\n",
    "dbfile = workDir + '/db/tweets.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the JSON file can be accessed\n",
    "try:\n",
    "    f = open(dbfile)\n",
    "except IOError:\n",
    "    print(\"File not accessible\")\n",
    "finally:\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@Deprecated\n",
    "def getCitiesPerState(state):\n",
    "    '''\n",
    "        @param state must begin with an uppercase letter\n",
    "        scrape the list of cities and towns in the respective state\n",
    "    '''\n",
    "    raise DeprecationWarning('Not working, sadly.')\n",
    "    cities = []\n",
    "    url = 'https://en.wikipedia.org/wiki/List_of_cities_and_towns_in_' + state\n",
    "    print('url to scrape:', url)\n",
    "    website = requests.get(url)\n",
    "    # load the retrieved site with BS\n",
    "    soup = BeautifulSoup(website.text,'html.parser')\n",
    "    \n",
    "    # cities/towns are stored in an element with class 'wikitable sortable'\n",
    "    cities_table = soup.find_all(\"table\", class_ = \"wikitable sortable\")\n",
    "    print(len(cities_table))\n",
    "    '''\n",
    "    cities_links = cities_table.findAll('a') # city names are links in the table\n",
    "    for link in cities_links:\n",
    "        cities.append(link.get('title'))\n",
    "    '''\n",
    "    return cities\n",
    "\"\"\"\n",
    "def readCitiesFromFile(state):\n",
    "    cities = []\n",
    "    with open(workDir + '/data/cities_' + state + '.txt') as txt:\n",
    "        for line in txt:\n",
    "            cities.append(line.strip().lower())\n",
    "\n",
    "    return pd.DataFrame(cities, columns = ['city'])\n",
    "\n",
    "def readKeywords():\n",
    "    keywords = []\n",
    "\n",
    "    with open(workDir + '/data/keywords.txt') as txt:\n",
    "        for line in txt:\n",
    "            keywords.append(line.strip().lower())\n",
    "\n",
    "    print(keywords)\n",
    "    return keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "city\n0        acres green\n1            aguilar\n2  air force academy\n3              akron\n4            alamosa\n5       alamosa east\n6         allenspark\n7               alma\n8           antonito\n9          applewood\n             city\n0  acalanes ridge\n1          acampo\n2           acton\n3        adelanto\n4            adin\n5    agoura hills\n6      agua dulce\n7         aguanga\n8        ahwahnee\n9         airport\n              city\n0         abington\n1  acushnet center\n2            adams\n3      agawam town\n4    amesbury town\n5   amherst center\n6          andover\n7        arlington\n8            athol\n9        attleboro\n"
    }
   ],
   "source": [
    "cities_colorado = readCitiesFromFile('co')\n",
    "cities_california = readCitiesFromFile('ca')\n",
    "cities_massachussetts = readCitiesFromFile('ma')\n",
    "cities_all = cities_california + cities_colorado + cities_massachussetts\n",
    "print(cities_colorado[0:10])\n",
    "print(cities_california[0:10])\n",
    "print(cities_massachussetts[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "20189 tweets loaded!\n"
    }
   ],
   "source": [
    "tweets = pd.read_csv(dbfile, names=['tweet_id', 'created', 'loc', 'text'], header=None)\n",
    "tweets = tweets[tweets['loc'].isnull() == False] # sort out NaN places\n",
    "tweets = tweets[tweets['loc'].str.contains('\\d') == False]\n",
    "tweets = tweets.applymap(lambda s:s.lower() if type(s) == str else s)\n",
    "print(len(tweets), 'tweets loaded!') # tweets having a location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tweet_id              created                       loc  \\\n0   1211029295977492481  2019-12-28t21:00:58       greater hartford ct   \n1   1211029297894326272  2019-12-28t21:00:59             providence ri   \n2   1211029301195358209  2019-12-28t21:00:59                  new york   \n3   1211029301791002624  2019-12-28t21:01:00           connecticut usa   \n5   1211029330740043777  2019-12-28t21:01:06          rhode island usa   \n6   1211029331738349569  2019-12-28t21:01:07                 boston ma   \n7   1211029338596028418  2019-12-28t21:01:08              waterbury ct   \n9   1211029342719004674  2019-12-28t21:01:09             charleston sc   \n10  1211029350621036545  2019-12-28t21:01:11                 lowell ma   \n11  1211029350910451712  2019-12-28t21:01:11          tampa fl she her   \n12  1211029356174282752  2019-12-28t21:01:13  dorchester massachusetts   \n14  1211029376206344193  2019-12-28t21:01:17            virginia beach   \n15  1211029377414303745  2019-12-28t21:01:18                shelton ct   \n16  1211029378672594944  2019-12-28t21:01:18         massachusetts usa   \n17  1211029456141332480  2019-12-28t21:01:36                        ny   \n\n                                                 text  \n0   see our latest hartford ct operations job oppo...  \n1   this job might be a great fit for you service ...  \n2    rkj65 and what s the sweater is it one or two...  \n3   i do not find this video funny this my face al...  \n5    johnleguizamo sad when celebrities have to tr...  \n6   this industrial loft is for sale go to https t...  \n7                           tttravie watch your mouth  \n9                  honestly tik toks make hours go by  \n10                     pulte pultedaily yes you re do  \n11  when i was little i was disgusted by my mom le...  \n12                        it s time for lsu football   \n14                          asia lily jdiane love you  \n15   blkmymorris fawfulfan everyone is missing poi...  \n16   jburton in style there must be a story that a...  \n17                                              facts  \n                  tweet_id              created                   loc  \\\n24978  1211694486906163200  2019-12-30t17:04:12           modesto ca    \n24979  1211694487027814403  2019-12-30t17:04:12        corona del mar   \n24981  1211694494589960192  2019-12-30t17:04:14    oakland california   \n24982  1211694494606708737  2019-12-30t17:04:14          santa ana ca   \n24983  1211694494917115913  2019-12-30t17:04:14                israel   \n24984  1211694499144990720  2019-12-30t17:04:15            losa ccusa   \n24985  1211694500596375566  2019-12-30t17:04:15  por todo l a califas   \n24986  1211694498910064640  2019-12-30t17:04:15        los angeles ca   \n24987  1211694504169951232  2019-12-30t17:04:16            sacramento   \n24988  1211694506187223040  2019-12-30t17:04:17              indio ca   \n24989  1211694506610835456  2019-12-30t17:04:17          las vegas nv   \n24990  1211694506656972806  2019-12-30t17:04:17          los banos ca   \n24991  1211694507210743809  2019-12-30t17:04:17      baldwinsville ny   \n24992  1211694508032659456  2019-12-30t17:04:17             they them   \n24993  1211694508808667138  2019-12-30t17:04:17         roseville ca    \n\n                                                    text  \n24978  as we end this year, we are highlighting the a...  \n24979  rocky is hanging out with big bro blue.goodman...  \n24981  huh. i hope this brings pivotal tracker (imo t...  \n24982  my ex bf just ordered a baby yoda pop! forgive...  \n24983  @seanhannity @ingrahamangle @lindseykevitch @f...  \n24984                                      open for shop  \n24985  @sign0fdoom this is how i meant to sing it to ...  \n24986  with that being said, im going back to sleep w...  \n24987  when i die girl im comin back 4 you, n imma pu...  \n24988  mexicans have a fridge in the garage just for ...  \n24989                          texas üá®üá± ‚ù§Ô∏è #2ndamendment  \n24990  maybe they feel \"unsafe\" cuz you might kick so...  \n24991  scenes from warm ups at pasadena city college ...  \n24992  @bookofborle @jameelajamil ive ruined a ton of...  \n24993                                       angry üò§ john  \n"
    }
   ],
   "source": [
    "print(tweets.head(15))\n",
    "print(tweets.tail(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['greater hartford ct' 'providence ri' 'new york' ... 'corona del mar'\n 'losa ccusa' 'por todo l a califas']\n"
    }
   ],
   "source": [
    "# How unique are the users locations?\n",
    "print(tweets['loc'].unique())\n",
    "#print(tweets['loc'].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet_id</th>\n      <th>created</th>\n      <th>loc</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>",
      "text/plain": "Empty DataFrame\nColumns: [tweet_id, created, loc, text]\nIndex: []"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for some real location names\n",
    "tweets[tweets['loc'].isin(cities_all) == True]\n",
    "# this approach is not working, due to the reason, that the city names in the location field are not even similar to the actual ones\n",
    "# e.g. due to misspellings or out of secrecy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet_id</th>\n      <th>created</th>\n      <th>loc</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6</th>\n      <td>1211029331738349569</td>\n      <td>2019-12-28t21:01:07</td>\n      <td>boston ma</td>\n      <td>this industrial loft is for sale go to https t...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>1211029350621036545</td>\n      <td>2019-12-28t21:01:11</td>\n      <td>lowell ma</td>\n      <td>pulte pultedaily yes you re do</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>1211029356174282752</td>\n      <td>2019-12-28t21:01:13</td>\n      <td>dorchester massachusetts</td>\n      <td>it s time for lsu football</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>1211029458678882304</td>\n      <td>2019-12-28t21:01:37</td>\n      <td>boston ma</td>\n      <td>met this angel outside of lana del rey s first...</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>1211029479809785857</td>\n      <td>2019-12-28t21:01:42</td>\n      <td>boston ma</td>\n      <td>eliselemassena go elise</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>24985</th>\n      <td>1211694500596375566</td>\n      <td>2019-12-30t17:04:15</td>\n      <td>por todo l a califas</td>\n      <td>@sign0fdoom this is how i meant to sing it to ...</td>\n    </tr>\n    <tr>\n      <th>24986</th>\n      <td>1211694498910064640</td>\n      <td>2019-12-30t17:04:15</td>\n      <td>los angeles ca</td>\n      <td>with that being said, im going back to sleep w...</td>\n    </tr>\n    <tr>\n      <th>24988</th>\n      <td>1211694506187223040</td>\n      <td>2019-12-30t17:04:17</td>\n      <td>indio ca</td>\n      <td>mexicans have a fridge in the garage just for ...</td>\n    </tr>\n    <tr>\n      <th>24990</th>\n      <td>1211694506656972806</td>\n      <td>2019-12-30t17:04:17</td>\n      <td>los banos ca</td>\n      <td>maybe they feel \"unsafe\" cuz you might kick so...</td>\n    </tr>\n    <tr>\n      <th>24993</th>\n      <td>1211694508808667138</td>\n      <td>2019-12-30t17:04:17</td>\n      <td>roseville ca</td>\n      <td>angry üò§ john</td>\n    </tr>\n  </tbody>\n</table>\n<p>7594 rows √ó 4 columns</p>\n</div>",
      "text/plain": "                  tweet_id              created                       loc  \\\n6      1211029331738349569  2019-12-28t21:01:07                 boston ma   \n10     1211029350621036545  2019-12-28t21:01:11                 lowell ma   \n12     1211029356174282752  2019-12-28t21:01:13  dorchester massachusetts   \n18     1211029458678882304  2019-12-28t21:01:37                 boston ma   \n23     1211029479809785857  2019-12-28t21:01:42                 boston ma   \n...                    ...                  ...                       ...   \n24985  1211694500596375566  2019-12-30t17:04:15      por todo l a califas   \n24986  1211694498910064640  2019-12-30t17:04:15            los angeles ca   \n24988  1211694506187223040  2019-12-30t17:04:17                  indio ca   \n24990  1211694506656972806  2019-12-30t17:04:17              los banos ca   \n24993  1211694508808667138  2019-12-30t17:04:17             roseville ca    \n\n                                                    text  \n6      this industrial loft is for sale go to https t...  \n10                        pulte pultedaily yes you re do  \n12                           it s time for lsu football   \n18     met this angel outside of lana del rey s first...  \n23                              eliselemassena go elise   \n...                                                  ...  \n24985  @sign0fdoom this is how i meant to sing it to ...  \n24986  with that being said, im going back to sleep w...  \n24988  mexicans have a fridge in the garage just for ...  \n24990  maybe they feel \"unsafe\" cuz you might kick so...  \n24993                                       angry üò§ john  \n\n[7594 rows x 4 columns]"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These tweets contain locations, either real or fake\n",
    "tweets[tweets['loc'].str.contains('|'.join([' ma', ' co', ' ca']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "72.20119867254445"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# average tweet length\n",
    "np.mean(tweets['text'].str.len())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['flu', 'kill', 'awful', 'virus', 'syndrome', 'sickness', 'disease', 'illness', 'ill', 'sick', 'virulent', 'medicine', 'influenza', 'vaccine', 'drug', 'cough', 'cure', 'diagnosis', 'medication', 'sore', 'throat', 'doctor', 'cold', 'stomach', 'ache', 'pain', 'tired', 'nausea']\nInt64Index([    0,     1,     2,     3,     5,     6,     7,     9,    10,\n               11,\n            ...\n            24984, 24985, 24986, 24987, 24988, 24989, 24990, 24991, 24992,\n            24993],\n           dtype='int64', length=20189)\nNo. of hits:  0\n"
    }
   ],
   "source": [
    "# tweet tokenization and stopword removal\n",
    "_stopwords = ['https', 'co', 'ct'] + stopwords.words('english')\n",
    "_keywords = readKeywords()\n",
    "def processTweet(tweet):\n",
    "    lem = WordNetLemmatizer()\n",
    "    tweet_ = tweet.lower() # 1. transform to lowercase\n",
    "    tweet_ = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', 'URL', tweet) # remove URLs\n",
    "    tweet_ = re.sub('@[^\\s]+', 'AT_USER', tweet) # remove usernames\n",
    "    tweet_ = re.sub(r'#([^\\s]+)', r'\\1', tweet) # remove the # in #hashtag\n",
    "    tweet_ = [word for word in word_tokenize(tweet) if word.isalpha() == True]\n",
    "    tweet_ = [lem.lemmatize(word,'v') for word in tweet]\n",
    "    return tweet_\n",
    "\n",
    "def tokenizeTweet(tweet):\n",
    "    return [word for word in tweet if word not in _stopwords]\n",
    "\n",
    "'''\n",
    "A vocabulary in Natural Language Processing is a list of all speech segments available for the model. In our case, this includes all the words resident in the Training set we have, as the model can make use of all of them relatively equally ‚Äî at this point, to say the least\n",
    "'''\n",
    "def buildVocab(tokenizedData):\n",
    "    wordlist = nltk.FreqDist(tokenizedData)\n",
    "    #wordlist.plot(50, cumulative = True)\n",
    "    word_features = wordlist.keys()\n",
    "    \n",
    "    return word_features\n",
    "\n",
    "def filterKeywords(tweet):\n",
    "    if any(x in processTweet(tweet) for x in _keywords):\n",
    "        #print(tweet)\n",
    "        return True\n",
    "    #if(tweet in _keywords):\n",
    "    #    print(tweet)\n",
    "\n",
    "filtered_tweets = pd.DataFrame(None) # creates an empty dataframe\n",
    "hits = 0\n",
    "#for tweet in tweets:\n",
    "print(tweets.index)\n",
    "for i in range(len(tweets)):\n",
    "    tweet = tweets.iloc[i]\n",
    "    if(filterKeywords(tweet['text']) == True):\n",
    "        hits += 1\n",
    "        tweet['text'] = processTweet(tweet['text'])\n",
    "        filtered_tweets = filtered_tweets.append(tweet)\n",
    "    #tweet_tokens = processTweet(tweet)\n",
    "    #for token in tweet_tokens:\n",
    "    #    processed_tweets.append(token)\n",
    "print('No. of hits: ', hits)\n",
    "#print(processed_tweets[0:10])\n",
    "#print(buildVocab(processed_tweets))\n",
    "#buildVocab(processed_tweets[:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'loc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/lfi/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'loc'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-c62b85999fa4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfiltered_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/lfi/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2994\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2995\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2996\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2997\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lfi/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2899\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2900\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'loc'"
     ]
    }
   ],
   "source": [
    "filtered_tweets['loc'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>created</th>\n      <th>loc</th>\n      <th>text</th>\n      <th>tweet_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>153</th>\n      <td>2019-12-28t21:03:50</td>\n      <td>worldwide lsu nation</td>\n      <td>[heart, heavy, carley, mccord, kill, morning, ...</td>\n      <td>1.211030e+18</td>\n    </tr>\n    <tr>\n      <th>347</th>\n      <td>2019-12-28t21:07:19</td>\n      <td>cold spring harbor ny</td>\n      <td>[andrewhires, mnitabach, ill, get, home, soon,...</td>\n      <td>1.211031e+18</td>\n    </tr>\n    <tr>\n      <th>382</th>\n      <td>2019-12-28t21:07:52</td>\n      <td>commonwealth of massachusetts</td>\n      <td>[tfw, sick, cold, remember, doctorwho, maratho...</td>\n      <td>1.211031e+18</td>\n    </tr>\n    <tr>\n      <th>387</th>\n      <td>2019-12-28t21:07:59</td>\n      <td>new york</td>\n      <td>[johncardillo, turn, stomach]</td>\n      <td>1.211031e+18</td>\n    </tr>\n    <tr>\n      <th>410</th>\n      <td>2019-12-28t21:08:32</td>\n      <td>in this bitch</td>\n      <td>[hey, friends, wednesday, shave, head, yesterd...</td>\n      <td>1.211031e+18</td>\n    </tr>\n    <tr>\n      <th>816</th>\n      <td>2019-12-28t21:16:14</td>\n      <td>east atl</td>\n      <td>[bitch, remind, da, time, dat, drink, kill, yo...</td>\n      <td>1.211033e+18</td>\n    </tr>\n    <tr>\n      <th>845</th>\n      <td>2019-12-28t21:16:50</td>\n      <td>pueblo co</td>\n      <td>[damn, joe, kill, benju, like, pussy]</td>\n      <td>1.211033e+18</td>\n    </tr>\n    <tr>\n      <th>873</th>\n      <td>2019-12-28t21:17:15</td>\n      <td>ca on city co</td>\n      <td>[king, axis, hp, avix, gb, artvii, nope, im, g...</td>\n      <td>1.211033e+18</td>\n    </tr>\n    <tr>\n      <th>1035</th>\n      <td>2019-12-28t21:19:56</td>\n      <td>probably with mollie</td>\n      <td>[zachhowell, didnt, picture, bad, ill, fix]</td>\n      <td>1.211034e+18</td>\n    </tr>\n    <tr>\n      <th>1124</th>\n      <td>2019-12-28t21:21:02</td>\n      <td>burlington co</td>\n      <td>[pdazzleliscious, theceng, actually, live, pai...</td>\n      <td>1.211034e+18</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                  created                            loc  \\\n153   2019-12-28t21:03:50          worldwide lsu nation    \n347   2019-12-28t21:07:19          cold spring harbor ny   \n382   2019-12-28t21:07:52  commonwealth of massachusetts   \n387   2019-12-28t21:07:59                       new york   \n410   2019-12-28t21:08:32                  in this bitch   \n816   2019-12-28t21:16:14                       east atl   \n845   2019-12-28t21:16:50                      pueblo co   \n873   2019-12-28t21:17:15                  ca on city co   \n1035  2019-12-28t21:19:56           probably with mollie   \n1124  2019-12-28t21:21:02                  burlington co   \n\n                                                   text      tweet_id  \n153   [heart, heavy, carley, mccord, kill, morning, ...  1.211030e+18  \n347   [andrewhires, mnitabach, ill, get, home, soon,...  1.211031e+18  \n382   [tfw, sick, cold, remember, doctorwho, maratho...  1.211031e+18  \n387                       [johncardillo, turn, stomach]  1.211031e+18  \n410   [hey, friends, wednesday, shave, head, yesterd...  1.211031e+18  \n816   [bitch, remind, da, time, dat, drink, kill, yo...  1.211033e+18  \n845               [damn, joe, kill, benju, like, pussy]  1.211033e+18  \n873   [king, axis, hp, avix, gb, artvii, nope, im, g...  1.211033e+18  \n1035        [zachhowell, didnt, picture, bad, ill, fix]  1.211034e+18  \n1124  [pdazzleliscious, theceng, actually, live, pai...  1.211034e+18  "
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filtered_tweets[filtered_tweets['loc'].str.contains(' ny')]\n",
    "filtered_tweets.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}